# â—† LLM Studio â€” Terminal-based LLM Playground

A **TUI (Terminal User Interface)** application for running, managing, and serving Large Language Models locally. Inspired by LM Studio, built entirely for the terminal.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â—† LLM Studio v1.0                                                     â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ğŸ  Home   â”‚  â”‚  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—   â”‚  â”‚
â”‚  â”‚ ğŸ’¬ Chat   â”‚  â”‚  â•‘     â—†  Welcome to LLM Studio  â—†            â•‘   â”‚  â”‚
â”‚  â”‚ ğŸ“¦ Models â”‚  â”‚  â•‘     Your Local LLM Playground               â•‘   â”‚  â”‚
â”‚  â”‚ ğŸŒ Server â”‚  â”‚  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚  â”‚
â”‚  â”‚ âš™ï¸ Config  â”‚  â”‚                                                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  [ğŸ“¦ Models: 3]  [ğŸŒ Server: OFF]  [ğŸ–¥ CPU: 8]    â”‚  â”‚
â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â—† No model loaded          Server: OFF              Ctrl+Q: Quit       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           TUI Frontend (Textual)         â”‚   â† Python + Rich/Textual
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Application Logic (Python)       â”‚   â† Config, Model Manager
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      API Server (FastAPI + Uvicorn)      â”‚   â† OpenAI-compatible REST API
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    Inference Engine (llama-cpp-python)    â”‚   â† C++ (llama.cpp) bindings
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **TUI/Interface** | Python + Textual | Beautiful terminal UI with mouse support |
| **Backend** | Python + FastAPI | Model management & API server |
| **Inference Engine** | llama.cpp (via Python bindings) | High-performance C++ LLM inference |
| **Model Format** | GGUF | Quantized model format for efficient CPU/GPU inference |

## Features

### ğŸ  Home Dashboard
- System overview with model count, server status, storage info
- Quick-action buttons for navigation

### ğŸ’¬ Interactive Chat
- Real-time streaming responses with token-by-token display
- Chat history management
- Configurable system prompt

### ğŸ“¦ Model Management
- **Browse local models** â€” list all downloaded GGUF files
- **Search HuggingFace** â€” find GGUF models from HF repos
- **Download models** â€” download directly with progress bar
- **Load/Unload** â€” manage model in memory
- **Delete models** â€” clean up storage

### ğŸŒ OpenAI-Compatible API Server
- Start/stop server from the TUI
- **Full OpenAI API compatibility:**
  - `POST /v1/chat/completions` (streaming supported)
  - `POST /v1/completions` (streaming supported)
  - `POST /v1/embeddings`
  - `GET /v1/models`
- Optional API key authentication
- CORS support
- Works with any OpenAI client library

### âš™ï¸ Settings
- Temperature, Top-P, Top-K, Repeat Penalty
- Context length, Max tokens
- CPU threads, GPU layers
- Batch size, Random seed
- System prompt
- Model storage directory
- All settings persisted to `~/.llm_studio/config.yaml`

## Installation

### 1. Clone & Install

```bash
cd "Day79 LLM Studio"
pip install -r requirements.txt
```

### 2. For GPU Support (Optional)

```bash
# CUDA support
CMAKE_ARGS="-DLLAMA_CUDA=on" pip install llama-cpp-python --force-reinstall --no-cache-dir

# Metal (macOS) support
CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python --force-reinstall --no-cache-dir
```

### 3. Install as Package (Optional)

```bash
pip install -e .
```

## Usage

### Launch the TUI

```bash
python run.py
# or if installed as package:
llm-studio
```

### Keyboard Shortcuts

| Key | Action |
|-----|--------|
| `F1` | Home screen |
| `F2` | Chat screen |
| `F3` | Models screen |
| `F4` | Server screen |
| `F5` | Settings screen |
| `Ctrl+Q` | Quit |
| `Ctrl+T` | Toggle dark/light theme |
| `Tab` | Navigate between elements |
| `Enter` | Send message / Activate button |

### Quick Start

1. **Launch** â†’ `python run.py`
2. **Go to Models** (F3) â†’ Download tab â†’ Enter a repo ID like `TheBloke/Mistral-7B-Instruct-v0.2-GGUF`
3. **Search** â†’ Select a quantization (e.g., Q4_K_M) â†’ Download
4. **Local Models** tab â†’ Select model â†’ Click "Load"
5. **Go to Chat** (F2) â†’ Start chatting!
6. **Optionally** â†’ Go to Server (F4) â†’ Start the OpenAI-compatible API

### Using the API Server

Once the server is running, use any OpenAI client:

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:1234/v1",
    api_key="not-needed",  # or your configured key
)

response = client.chat.completions.create(
    model="local-model",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"},
    ],
    stream=True,
)

for chunk in response:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")
```

Or with `curl`:

```bash
curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "local-model",
    "messages": [{"role": "user", "content": "Hello!"}],
    "stream": false
  }'
```

## Project Structure

```
Day79 LLM Studio/
â”œâ”€â”€ run.py                          # Entry point
â”œâ”€â”€ setup.py                        # Package setup
â”œâ”€â”€ requirements.txt                # Dependencies
â”œâ”€â”€ README.md
â””â”€â”€ llm_studio/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ app.py                      # Main TUI application
    â”œâ”€â”€ config.py                   # Configuration management
    â”œâ”€â”€ models/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ manager.py              # Model download/list/delete
    â”‚   â””â”€â”€ engine.py               # Inference engine (llama.cpp)
    â”œâ”€â”€ server/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â””â”€â”€ api.py                  # OpenAI-compatible REST API
    â””â”€â”€ ui/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ widgets/
        â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”œâ”€â”€ sidebar.py          # Navigation sidebar
        â”‚   â”œâ”€â”€ message_list.py     # Chat message bubbles
        â”‚   â””â”€â”€ status_bar.py       # Bottom status bar
        â”œâ”€â”€ screens/
        â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”œâ”€â”€ home.py             # Dashboard
        â”‚   â”œâ”€â”€ chat.py             # Chat interface
        â”‚   â”œâ”€â”€ models.py           # Model management
        â”‚   â”œâ”€â”€ server.py           # API server control
        â”‚   â””â”€â”€ settings.py         # Configuration
        â””â”€â”€ styles/
            â””â”€â”€ app.tcss            # Textual CSS styles
```

## Configuration

Settings are stored in `~/.llm_studio/config.yaml`:

```yaml
models_dir: ~/.llm_studio/models
theme: dark
system_prompt: You are a helpful AI assistant.
inference:
  n_ctx: 4096
  n_threads: 4
  n_gpu_layers: 0
  temperature: 0.7
  top_p: 0.9
  top_k: 40
  repeat_penalty: 1.1
  max_tokens: 2048
  n_batch: 512
  seed: -1
server:
  host: 0.0.0.0
  port: 1234
  api_key: null
  cors_origins:
    - "*"
```

## Requirements

- Python 3.10+
- A terminal with Unicode support (most modern terminals)
- GGUF model files (download via the app or manually place in `~/.llm_studio/models/`)

## License

MIT
